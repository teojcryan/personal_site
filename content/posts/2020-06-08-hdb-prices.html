---
title: Determinants of Public Housing Prices in Singapore
author: Ryan Teo
date: '2020-05-05'
slug: hdb-prices
categories: ["statistical-learning"]
tags: ["hdb"]
bibliography: [2020-06-08-hdb-prices.bib]
link-citations: true
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="summary" class="section level1">
<h1>Summary</h1>
<p>This objective of this study was to develop predictive models for HDB resale prices by fitting transaction records augmented with geospatial data on various statistical learning models and assessing variable importance. Ensemble methods such as gradient boosting machines (GBMs), bagged trees and random forest had the best predictive ability, while neural networks had the lowest predictive ability. In terms of the relative importance of resale price determinants, the study found that housing characteristics like floor area and remaining lease were more important than most locational factors except for the distance to CBD and the nearest MRT station.</p>
</div>
<div id="introduction" class="section level1">
<h1>1 Introduction</h1>
<p>The vast majority of Singaporean households live in public housing apartments built by the Housing Development Board (HDB). These flats are built in different configurations of relatively homogenous layouts across Singapore and can be sold on an open market after a five-year minimum occupation period <span class="citation">(<a href="#ref-hdb_minimum_2018" role="doc-biblioref">HDB 2018</a>)</span>. Housing prices are often modelled by hedonic modelling, which considers a building as a “bundle of goods” encompassing its physical and locational characteristics <span class="citation">(<a href="#ref-monson_valuation_2009" role="doc-biblioref">Monson 2009</a>)</span>, the value of which determine housing prices. HDB resale flats are prime subjects of such hedonic analysis since the consistency in structure quality across neighbourhoods helps to distinguish price changes due to unit-specific and locational determinants <span class="citation">(<a href="#ref-xu_hedonic_2017" role="doc-biblioref">Xu 2017</a>)</span>.</p>
<p>Using HDB resale transactions in 2019 augmented with geospatial data, this study explores two main objectives: first, to predict HDB resale prices by fitting various statistical learning models; second, to infer the relative importance of predictors from these models. The outcome of the study could help potential resale flat buyers and sellers navigate the pricing market.The <a href="#methodology">next section</a> describes the datasets, statistical learning models and metrics used. <a href="#results">Section 3</a> summarises the results, and <a href="#discussion">Section 4</a> discusses the possible implications of this study before <a href="#conclusion">Section 5</a> concludes the report.</p>
</div>
<div id="methodology" class="section level1">
<h1>2 Methodology</h1>
<div id="dataset" class="section level2">
<h2>2.1 Dataset</h2>
<p>The dataset was sourced from the government’s open data portal. It contains data for all HDB resale flat transactions in 2019 with a total of 22,189 entries <span class="citation">(<a href="#ref-hdb_resale_2020" role="doc-biblioref">HDB 2020</a>)</span>. Each entry includes information on the nominal transaction price, the town where the flat is situated, the flat layout and model, address, storey, floor area and length of lease remaining. The dataset was then geocoded based on the addresses using the OneMap API <span class="citation">(<a href="#ref-sla_new_2018" role="doc-biblioref">SLA 2018</a>)</span>, and the geodesic distance from the flats to the Central Business District (CBD) and nearby amenities (e.g. nearest train station, hawker centre) was computed. The dataset did not contain missing values. A summary of all relevant variables is included below.</p>
<table>
<colgroup>
<col width="33%" />
<col width="66%" />
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Resale Price</td>
<td>Nominal price of transaction</td>
</tr>
<tr class="even">
<td>Remaining Lease</td>
<td>Length of remaining lease in years</td>
</tr>
<tr class="odd">
<td>Floor area</td>
<td>Floor area in m2</td>
</tr>
<tr class="even">
<td>Storey</td>
<td>Storey of unit</td>
</tr>
<tr class="odd">
<td>Town</td>
<td>Town in which flat was located</td>
</tr>
<tr class="even">
<td>Flat model</td>
<td>Model of flat (e.g. new generation)</td>
</tr>
<tr class="odd">
<td>Flat type</td>
<td>Number of rooms in flat (e.g. 3-room)</td>
</tr>
<tr class="even">
<td>Distance to CBD</td>
<td>Geodesic distance from unit to CBD in m</td>
</tr>
<tr class="odd">
<td>Minimum distance (mall)</td>
<td>Geodesic distance to nearest shopping mall in m</td>
</tr>
<tr class="even">
<td>Minimum distance (public hospital)</td>
<td>Geodesic distance to nearest public hospital in m</td>
</tr>
<tr class="odd">
<td>Minimum distance (MRT station)</td>
<td>Geodesic distance to nearest MRT station in m</td>
</tr>
<tr class="even">
<td>Minimum distance (primary school)</td>
<td>Geodesic distance to nearest top 25 primary school in m</td>
</tr>
<tr class="odd">
<td>Minimum distance (fire station)</td>
<td>Geodesic distance to nearest fire station in m</td>
</tr>
<tr class="even">
<td>Minimum distance (police station)</td>
<td>Geodesic distance to nearest Singapore Police Force establishment in m</td>
</tr>
<tr class="odd">
<td>Minimum distance (waste treatment facility)</td>
<td>Geodesic distance to nearest toxic industrial wastes treatment and disposal facility in m</td>
</tr>
<tr class="even">
<td>Minimum distance (park)</td>
<td>Geodesic distance to nearest park in m</td>
</tr>
<tr class="odd">
<td>Minimum distance (community club)</td>
<td>Geodesic distance to nearest community club in m</td>
</tr>
<tr class="even">
<td>Minimum distance (public library)</td>
<td>Geodesic distance to nearest public library in m</td>
</tr>
<tr class="odd">
<td>Minimum distance (exercise facility)</td>
<td>Geodesic distance to nearest public exercise facility in m</td>
</tr>
<tr class="even">
<td>Minimum distance (hawker centre)</td>
<td>Geodesic distance to nearest hawker centre in m</td>
</tr>
</tbody>
</table>
</div>
<div id="pre-processing" class="section level2">
<h2>2.2 Pre-Processing</h2>
<p>The Variance Influence Factor (VIF) and the correlation matrix showed that some groups of variables were correlated with each other. If left unresolved, multicollinearity may cause misleading results about variable importance <span class="citation">(<a href="#ref-paul_multicollinearity_2006" role="doc-biblioref">Paul 2006</a>)</span>. In particular, town was correlated with the distance to the CBD, and flat type was correlated with floor area. This was expected since both sets of variables provide similar information. A conscious decision was made to drop the town, flat type, and model variables to improve model conciseness and interpretability. All remaining variables had a VIF between 1 to 3, which indicated that multicollinearity was largely reduced. No further feature selection was conducted since variable importance measures would reveal insignificant features. Prior to model fitting, all predictor variables were centred and scaled to aid comparisons across different units.</p>
</div>
<div id="models" class="section level2">
<h2>2.3 Models</h2>
<p>The dataset was randomly split into a training and test set with a 3:1 ratio. All models were fit and tuned using 4-fold cross validation on the training set and evaluated on the test set.</p>
<div id="multivariate-adaptive-regression-splines-mars" class="section level3">
<h3>2.3.1 Multivariate Adaptive Regression Splines (MARS)</h3>
<p>MARS is a non-parametric regression technique that extends linear models by modelling nonlinearities and interactions between variables through the weighted sum of basis functions. It is more flexible than linear regression and does automatic variable selection. However, the variable selection can sometimes be arbitrary when the predictors are correlated. To address this, the MARS models were bootstrapped aggregated. MARS models have a specific variable importance metric that tracks the reduction in generalized cross-validation estimate of error as each predictor is removed during the backwards elimination feature.</p>
</div>
<div id="gradient-boosting-machine-gbm" class="section level3">
<h3>2.3.2 Gradient Boosting Machine (GBM)</h3>
<p>GBMs build an ensemble of individual decision trees by iteratively increasing the weight of trees with high errors. This method boasts strong accuracy and flexibility, although it is computationally expensive. GBMs have a specific variable importance metric. This calculates the MSE for each tree on the out-of-bag (OOB) portion of the data after permuting each predictor variable. The differences are averaged and normalised by the standard error. Summing the importance values over each boosting iteration returns the overall variable importance <span class="citation">(<a href="#ref-kuhn_variable_2019" role="doc-biblioref">Kuhn 2019</a>)</span>.</p>
</div>
<div id="tree-bagging-random-forest" class="section level3">
<h3>2.3.3 Tree Bagging, Random Forest</h3>
<p>Decision tree bagging and random forests are tree ensemble methods similar to the GBM, except they weigh all trees equally. Random forests extend bagging by only taking a random subset of predictors to fit the trees. These methods also have the same variable importance measure as GBMs, calculated by the average increase in squared OOB residuals when the variable is permuted <span class="citation">(<a href="#ref-gromping_variable_2009" role="doc-biblioref">Grömping 2009</a>)</span>.</p>
</div>
<div id="neural-network" class="section level3">
<h3>2.3.4 Neural Network</h3>
<p>Basic feedforward neural networks were also chosen to model the resale prices. Neural networks are flexible but come at a cost of high variance as they learn via stochastic training algorithms. This makes them sensitive to changes in the training data which affects the weight calibration each time they are trained, leading to different predictions. To reduce the variance of the model, bagging was also applied to develop a model averaged neural network.
Unlike the preceding methods, neural networks do not have a model-specific measure of variable importance. As such, relative importance is evaluated by the <span class="math inline">\(R^2\)</span> statistic when a loess smoother is fit between the outcome and each individual predictor <span class="citation">(<a href="#ref-kuhn_variable_2019" role="doc-biblioref">Kuhn 2019</a>)</span>.</p>
</div>
</div>
<div id="metrics" class="section level2">
<h2>2.4 Metrics</h2>
<p>The metrics for the objective of predicting resale prices are the Root Mean Squared Error (RMSE) and R2 on the test set. These are commonly used metrics for regression problems and reflect the quality of fit and predictive ability of each model.</p>
<p>The metric for evaluating relative variable importance is less straightforward since it is measured on different scales between models and is therefore difficult to compare. For this study I proposed two possible solutions: first, a rank-based system that averages the rank of each predictor’s relative importance across all models; and second, a proportion-based score system that averaging the standardised relative importance across all models. The first method highlights the order of relative importance, while the second is a rough (albeit non-rigorous) estimate of the extent of relative importance.</p>
</div>
</div>
<div id="results" class="section level1">
<h1>3 Results</h1>
<p>The optimal hyperparameters from model tuning are summarised below.</p>
<center>
<table>
<colgroup>
<col width="19%" />
<col width="80%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Model</th>
<th align="left">Tuned Hyperparameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">MARS</td>
<td align="left">nprune = 34, degree = 2</td>
</tr>
<tr class="even">
<td align="left">Bagged MARS</td>
<td align="left">nprune = 100 and degree = 2</td>
</tr>
<tr class="odd">
<td align="left">GBM</td>
<td align="left">n.trees = 5000, interaction.depth = 5, shrinkage = 0.05, n.minobsinnode = 3</td>
</tr>
<tr class="even">
<td align="left">Bagged Trees</td>
<td align="left">mtry = 16</td>
</tr>
<tr class="odd">
<td align="left">Random Forest</td>
<td align="left">mtry = 6</td>
</tr>
<tr class="even">
<td align="left">Neural Network</td>
<td align="left">size = 5, decay = 0.01</td>
</tr>
<tr class="odd">
<td align="left">Model Averaged NN</td>
<td align="left">size = 3, decay = 0.1, bag = TRUE</td>
</tr>
</tbody>
</table>
</center>
<div id="prediction" class="section level2">
<h2>3.1 Prediction</h2>
<center>
<table>
<thead>
<tr class="header">
<th align="left">Model</th>
<th>RMSE</th>
<th><span class="math inline">\(R^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">MARS</td>
<td>47200</td>
<td>0.902</td>
</tr>
<tr class="even">
<td align="left">Bagged MARS</td>
<td>45200</td>
<td>0.910</td>
</tr>
<tr class="odd">
<td align="left">GBM</td>
<td>32800</td>
<td>0.953</td>
</tr>
<tr class="even">
<td align="left">Bagged Trees</td>
<td>19300</td>
<td>0.984</td>
</tr>
<tr class="odd">
<td align="left">Random Forest</td>
<td>19500</td>
<td>0.984</td>
</tr>
<tr class="even">
<td align="left">Neural Network</td>
<td>135000</td>
<td>0.446</td>
</tr>
<tr class="odd">
<td align="left">Model Averaged NN</td>
<td>114000</td>
<td>0.489</td>
</tr>
</tbody>
</table>
</center>
<p>The performance of the models is summarised in above. In general, bagged models (MARS and Model Averaged NN) improved over their singular counterparts, suggesting that the reduction in variance has aided predictive ability.
Bagged trees and random forest performed the best out of all the models, with bagging slightly outperforming random forest. Neural networks were found to have high errors, which was only slightly improved during bagging. Due to their low quality of model fit, neural networks were excluded in the variable importance computation.</p>
</div>
<div id="variable-importance" class="section level2">
<h2>3.2 Variable Importance</h2>
<center>
<table>
<thead>
<tr class="header">
<th align="left">Variable</th>
<th align="center">Rank</th>
<th align="center">Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Floor area (<span class="math inline">\(m^2\)</span>)</td>
<td align="center">1</td>
<td align="center">31.51</td>
</tr>
<tr class="even">
<td align="left">Distance to CBD</td>
<td align="center">2</td>
<td align="center">19.03</td>
</tr>
<tr class="odd">
<td align="left">Remaining lease</td>
<td align="center">3</td>
<td align="center">12.71</td>
</tr>
<tr class="even">
<td align="left">Minimum distance to nearest MRT station</td>
<td align="center">4</td>
<td align="center">6.41</td>
</tr>
<tr class="odd">
<td align="left">Storey</td>
<td align="center">4</td>
<td align="center">6.01</td>
</tr>
<tr class="even">
<td align="left">Minimum distance to nearest mall</td>
<td align="center">6</td>
<td align="center">5</td>
</tr>
<tr class="odd">
<td align="left">Minimum distance to nearest hospital</td>
<td align="center">6</td>
<td align="center">3.71</td>
</tr>
<tr class="even">
<td align="left">Minimum distance to nearest public library</td>
<td align="center">6</td>
<td align="center">2.09</td>
</tr>
<tr class="odd">
<td align="left">Minimum distance to nearest public gym</td>
<td align="center">9</td>
<td align="center">1.89</td>
</tr>
<tr class="even">
<td align="left">Minimum distance to nearest fire station</td>
<td align="center">9</td>
<td align="center">1.56</td>
</tr>
<tr class="odd">
<td align="left">Minimum distance to nearest park</td>
<td align="center">11</td>
<td align="center">2.39</td>
</tr>
<tr class="even">
<td align="left">Minimum distance to nearest primary school</td>
<td align="center">11</td>
<td align="center">1.56</td>
</tr>
<tr class="odd">
<td align="left">Minimum distance to nearest community centre</td>
<td align="center">11</td>
<td align="center">1.63</td>
</tr>
<tr class="even">
<td align="left">Minimum distance to nearest waste treatment plant</td>
<td align="center">14</td>
<td align="center">1.49</td>
</tr>
<tr class="odd">
<td align="left">Minimum distance to nearest police station</td>
<td align="center">14</td>
<td align="center">1.78</td>
</tr>
<tr class="even">
<td align="left">Minimum distance to nearest hawker centre</td>
<td align="center">15</td>
<td align="center">1.21</td>
</tr>
</tbody>
</table>
</center>
<p>The relative importance ranks and scores were computed on the bagged MARS, GBM, and random forest models and are shown in the tables above. While the bagged trees outperformed the random forest in terms of RMSE, the random forest model was selected due to the lower possibility of correlated trees and dominant predictors that are otherwise unimportant <span class="citation">(<a href="#ref-auret_empirical_2011" role="doc-biblioref">Auret and Aldrich 2011</a>)</span>. The results were generally consistent for the 6 most important variables, which suggested that housing characteristics were more important than most locational characteristics in determining HDB resale prices, with the exception of the distance to CBD and the nearest MRT station.</p>
<p>It should be noted that the variable importance rank and score vary slightly for less important factors, which suggest some underlying flaws in the score computation even if the predictors remain in the same tier of importance. Nonetheless, the models provided some idea of the order and extent of relative importance. Further information on the direction of influence by each predictor is represented in the partial dependence plots below.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p><img src="/post/2020-06-08-hdb-prices.png" /></p>
</div>
</div>
<div id="discussion" class="section level1">
<h1>4 Discussion</h1>
<p>The models highlighted in this study were chosen because most of them had specific variable importance measures. Furthermore, the models generally high predictive abilities. Ensemble models performed well since they have high flexibility in representing hypotheses not necessarily contained in the hypothesis space of the models from which they were built <span class="citation">(<a href="#ref-anderson_pattern_2019" role="doc-biblioref">Anderson 2019</a>)</span>. Neural networks were not expected to fare poorly, since they are known to be flexible, low bias methods. Model averaging only served to reduce the variance and not bias. A possible explanation for their high bias is that the model was underfitting or the optimal hyperparameters were outside the tune grid. This can be corrected by increasing the scope of the grid search, possibly reducing underfitting that led to high bias.</p>
<p>The results showed that of all predictors considered, housing characteristics like floor area and remaining lease were more important than most locational factors. This suggests that housing characteristics relate more directly to the value of the property, while amenities such as parks and hawker centres, which are commonly found across Singapore and are thus not unique, are valued less. A limitation of this method is that it considers amenities as homogeneous in value, when in reality there may be a higher premium for flats near for example an MRT interchange than less accessible station. These nuances are not captured in this study but may be considered in future studies.</p>
<p>This study can be also extended by including additional parameters such as time of transaction which may provide additional information about HDB resale price determinants. However, collinearity is still an issue that needs to be checked when introducing new variables.
Additionally, given more resources, models like the neural networks can be tuned more effectively or more models can be fit. For instance, generalised linear models lend themselves to more direct interpretation of the extent and direction of variable importance by the estimated <span class="math inline">\(\beta\)</span>’s, although the normality assumption has to be met.</p>
</div>
<div id="conclusion" class="section level1">
<h1>5 Conclusion</h1>
<p>This study sought to develop predictive models for HDB resale prices by fitting transaction records augmented with geospatial data on various statistical learning models and assessing variable importance. Ensemble methods such as GBMs, bagged trees and random forest had the best predictive ability, while neural networks had the lowest predictive ability. In terms of the relative importance of resale price determinants, the study concluded that housing characteristics like floor area and remaining lease were more important than most locational factors except for the distance to CBD and the nearest MRT station.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-anderson_pattern_2019" class="csl-entry">
Anderson, Brett. 2019. <em>Pattern <span>Recognition</span>: <span>An</span> Introduction</em>. Scientific e-Resources.
</div>
<div id="ref-auret_empirical_2011" class="csl-entry">
Auret, Lidia, and Chris Aldrich. 2011. <span>“Empirical Comparison of Tree Ensemble Variable Importance Measures.”</span> <em>Chemometrics and Intelligent Laboratory Systems</em> 105 (2): 157–70. <a href="https://doi.org/10.1016/j.chemolab.2010.12.004">https://doi.org/10.1016/j.chemolab.2010.12.004</a>.
</div>
<div id="ref-gromping_variable_2009" class="csl-entry">
Grömping, Ulrike. 2009. <span>“Variable <span>Importance</span> <span>Assessment</span> in <span>Regression</span>: <span>Linear</span> <span>Regression</span> Versus <span>Random</span> <span>Forest</span>.”</span> <em>The American Statistician</em> 63 (4): 308–19. <a href="https://doi.org/10.1198/tast.2009.08199">https://doi.org/10.1198/tast.2009.08199</a>.
</div>
<div id="ref-hdb_minimum_2018" class="csl-entry">
HDB. 2018. <span>“Minimum <span>Occupation</span> <span>Period</span> <span>Eligibility</span>.”</span> <em>Housing Development Board (HDB)</em>. <a href="https://www.hdb.gov.sg/cs/infoweb/residential/selling-a-flat/eligibility">https://www.hdb.gov.sg/cs/infoweb/residential/selling-a-flat/eligibility</a>.
</div>
<div id="ref-hdb_resale_2020" class="csl-entry">
———. 2020. <span>“Resale <span>Flat</span> <span>Prices</span>.”</span> <em>Data.gov.sg</em>. <a href="https://data.gov.sg/dataset/resale-flat-prices">https://data.gov.sg/dataset/resale-flat-prices</a>.
</div>
<div id="ref-kuhn_variable_2019" class="csl-entry">
Kuhn, Max. 2019. <em>Variable <span>Importance</span></em>. The Caret Package. <a href="https://topepo.github.io/caret/variable-importance.html">https://topepo.github.io/caret/variable-importance.html</a>.
</div>
<div id="ref-monson_valuation_2009" class="csl-entry">
Monson, Matt. 2009. <span>“Valuation <span>Using</span> <span>Hedonic</span> <span>Pricing</span> <span>Models</span>.”</span> <em>Cornell Real Estate Review</em> 7 (1). <a href="https://scholarship.sha.cornell.edu/crer/vol7/iss1/10">https://scholarship.sha.cornell.edu/crer/vol7/iss1/10</a>.
</div>
<div id="ref-paul_multicollinearity_2006" class="csl-entry">
Paul, Ranjit Kumar. 2006. <span>“Multicollinearity: <span>Causes</span>, <span>Effects</span>, and <span>Remedies</span>.”</span> <em>IASRI, New Delhi</em>, 58–65.
</div>
<div id="ref-sla_new_2018" class="csl-entry">
SLA. 2018. <span>“New <span>OneMap</span> <span>API</span> <span>Docs</span>.”</span> <a href="https://docs.onemap.sg/#introduction">https://docs.onemap.sg/#introduction</a>.
</div>
<div id="ref-xu_hedonic_2017" class="csl-entry">
Xu, Jiakun. 2017. <span>“Hedonic <span>Modeling</span> of <span>Singapore</span>’s <span>Resale</span> <span>Public</span> <span>Housing</span> <span>Market</span>.”</span> Thesis. <a href="https://dukespace.lib.duke.edu/dspace/handle/10161/14268">https://dukespace.lib.duke.edu/dspace/handle/10161/14268</a>.
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The image looks more clear if you open it in a new tab<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
